Author: Himaja Arabati
Student ID: 700772489

This repository contains three implementations related to NLP sequence modeling and attention mechanisms.

1ï¸âƒ£ Character-Level RNN Language Model

File: char_rnn_language_model.py

Description

Builds a simple character-level language model using RNN / LSTM / GRU.

Loads text, encodes characters, trains model to predict next character.

Generates text samples after training.

Features

Custom dataset preparation

Temperature-based sampling

Train + inference functions

Easy to extend for experiments

2ï¸âƒ£ Bigram Language Model

File: bigram_language_model.py

Description

Implements a basic bigram model from scratch.

Learns probability of each character given the previous character.

Features

Frequency counting

Bigram probability matrix

Random text generation using learned distributions

Very fast + easy to understand baseline

3ï¸âƒ£ Scaled Dot-Product Attention

File: scaled_dot_product_attention.py

Description

Implements the core attention formula used in Transformers:

Attention
(
ğ‘„
,
ğ¾
,
ğ‘‰
)
=
softmax
(
ğ‘„
ğ¾
ğ‘‡
ğ‘‘
ğ‘˜
)
ğ‘‰
Attention(Q,K,V)=softmax(
d
k
	â€‹

	â€‹

QK
T
	â€‹

)V

Uses random Q, K, V tensors for testing.

Features

Computes attention weights

Computes attention outputs

Prints pre-scaling and post-scaling logits to show softmax stability

Clean, well-commented PyTorch implementation

ğŸ“‚ Project Structure
.
â”œâ”€â”€ char_rnn_language_model.py
â”œâ”€â”€ bigram_language_model.py
â”œâ”€â”€ scaled_dot_product_attention.py
â””â”€â”€ README.md

â–¶ï¸ How to Run
python3 char_rnn_language_model.py
python3 bigram_language_model.py
python3 scaled_dot_product_attention.py
